# SPDX-FileCopyrightText: Copyright (c) 2025-2026, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# =============================================================================
# FULL EVALUATION - NO RETHINKING (100 scenarios)
# =============================================================================
# Purpose: Production benchmark evaluation without self-evaluation loop
# Dataset: 100 banking scenarios from Agent Leaderboard v2
#
# This configuration runs a standard ReAct agent without the self-evaluating
# wrapper, providing baseline TSQ scores for comparison. Use this to:
# - Establish baseline performance metrics
# - Benchmark Dynamo throughput and latency
# - Compare against rethinking-enabled configurations
#
# Usage:
#   nat eval --config_file configs/eval_config_no_rethinking_full_test.yml
#
# Expected runtime: ~30-60 minutes depending on concurrency
# Expected TSQ: 0.4 - 0.7
# =============================================================================
# <!-- path-check-skip-file -->

functions:
  react_benchmark_agent:
    _type: react_benchmark_agent
    prefix: "Agent:"
    decision_only: true
    canned_response_template: "Successfully executed {tool_name}. Operation completed."

function_groups:
  banking_tools:
    _type: banking_tools_group
    # tools.json available after running: /examples/dynamo_integration/scripts/download_agent_leaderboard_v2.py
    tools_json_path: ./examples/dynamo_integration/data/raw/banking/tools.json
    decision_only: true
    include: [
      get_account_balance,
      get_transaction_history,
      transfer_funds,
      get_loan_information,
      get_credit_card_information,
      get_mortgage_details,
      get_savings_account_products,
      schedule_appointment,
      check_loan_application_status,
      find_nearby_locations,
      get_investment_products,
      report_lost_stolen_card,
      update_contact_information,
      setup_automatic_bill_pay,
      initiate_transaction_dispute,
      get_exchange_rates,
      calculate_loan_payment,
      manage_account_alerts,
      check_wire_transfer_status,
      get_cd_products
    ]

llms:
  dynamo_llm:
    _type: dynamo
    model_name: llama-3.3-70b
    base_url: http://localhost:8099/v1
    api_key: dummy
    # _type: nim
    # model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0
    max_tokens: 8192
    stop: ["Observation:", "\nThought:"]  # CRITICAL: Prevent LLM from hallucinating observations
    # Dynamo prefix headers for KV cache optimization (enabled when prefix_template is set)
    prefix_template: "react-benchmark-{uuid}"  # Template for prefix IDs ({uuid} replaced per request)
    prefix_total_requests: 10  # 1 for independent questions, higher for conversations
    prefix_osl: MEDIUM  # Output Sequence Length: LOW | MEDIUM | HIGH
    prefix_iat: MEDIUM  # Inter-Arrival Time: LOW | MEDIUM | HIGH
    # # Optimizer: Parameters that can be tuned during optimization
    # optimizable_params:
    #   - temperature
    # # Optimizer: Search space for tunable parameters
    # search_space:
    #   temperature:
    #     low: 0.0
    #     high: 0.25
    #     step: 0.05
  
  eval_llm:
    _type: dynamo
    model_name: llama-3.3-70b
    base_url: http://localhost:8099/v1
    api_key: dummy
    # _type: nim
    # model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0
    max_tokens: 1024

workflow:
  _type: react_agent
  llm_name: dynamo_llm
  tool_names: [
    banking_tools
  ]
  verbose: true  # Enable to see tool calls
  parse_agent_response_max_retries: 3
  max_tool_calls: 25  # Increased from 15
  max_history: 1000
  pass_tool_call_errors_to_agent: true  # Let agent see tool errors
  recursion_limit: 50  # Increased from default 32
  # Custom system prompt that prevents observation hallucination
  system_prompt: |
    You are a tool-calling agent evaluated on TOOL SELECTION capability. Your goal is to select the correct tools, in the correct order, to handle real-world use-cases.
    
    IMPORTANT: This is a tool selection exercise, NOT real execution.
    - Focus on selecting the RIGHT TOOL for each step
    - Use placeholder or dummy values for required parameters (e.g., "12345", "user@example.com", "2024-01-01")
    - Tool responses are simulated - ignore them and focus on selecting the next appropriate tool
    - What matters is YOUR INTENT and TOOL CHOICE, not the data quality
    
    Available tools:

    {tools}

    Use this exact format for EACH response:

    Thought: I need to analyze what the user needs and select the SINGLE NEXT tool to call
    Action: the ONE tool to call right now, must be one of [{tool_names}]
    Action Input: valid JSON with required parameters (use placeholder values)

    CRITICAL RULES:
    1. Output ONLY ONE Thought, Action, and Action Input per response
    2. STOP IMMEDIATELY after writing Action Input
    3. DO NOT write the Observation - the system will provide it
    4. DO NOT write multiple Thought/Action/Action Input cycles in one response
    5. After receiving the Observation, you will get another turn to select the next tool
    
    When you have called all necessary tools:
    Thought: I now know the final answer
    Final Answer: [brief summary of what was accomplished]

eval:
  general:
    max_concurrency: 36 # range from 2 - 64
    
    output:
      dir: ./examples/dynamo_integration/react_benchmark_agent/outputs/dynamo_evals/no_rethinking_full_test/
      cleanup: false
      job_management:
        append_job_id_to_output_dir: true
    
    dataset:
      _type: json
      file_path: ./examples/dynamo_integration/data/agent_leaderboard_v2_banking.json
      structure:
        disable: true
    
    # Minimal profiler - disable problematic features
    profiler:
      compute_llm_metrics: false  # Disabled - missing columns issue
      token_uniqueness_forecast: false
      workflow_runtime_forecast: false
      prompt_caching_prefixes:
        enable: false
      bottleneck_analysis:
        enable_nested_stack: false
      concurrency_spike_analysis:
        enable: false

  evaluators:
    # TSQ only - no trajectory evaluator
    tool_selection_quality:
      _type: tsq_evaluator
      llm_name: eval_llm
      strict_mode: false
      tool_weight: 1.0
      parameter_weight: 0.0
      verbose: true

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
# Run with: nat optimize --config_file <this_file>
# 
# This section allows you to run parameter optimization on the same evaluation
# dataset and compare performance using the tsq_evaluator metrics.
# =============================================================================
optimizer:
  # Output directory for optimization results
  output_path: ./examples/dynamo_integration/react_benchmark_agent/outputs/dynamo_evals/no_rethinking_full_test/optimizer_results/
  
  # Number of repetitions per parameter set for stable evaluation
  reps_per_param_set: 1
  
  # Evaluation metrics to optimize (references the evaluators defined above)
  eval_metrics:
    tool_selection_quality:
      evaluator_name: tool_selection_quality  # References the tsq_evaluator
      direction: maximize                     # Higher TSQ score is better
      weight: 1.0
  
  # Numeric parameter optimization (Optuna-based)
  numeric:
    enabled: true
    sampler: grid            # Use grid search for systematic comparison
  
  # Prompt optimization (Genetic Algorithm) - disabled by default
  prompt:
    enabled: false
    # Uncomment below to enable prompt optimization:
    # prompt_population_init_function: prompt_init
    # ga_generations: 3
    # ga_population_size: 4
    # ga_parallel_evaluations: 1
