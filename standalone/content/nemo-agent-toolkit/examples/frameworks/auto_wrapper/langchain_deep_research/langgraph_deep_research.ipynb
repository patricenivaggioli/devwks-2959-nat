{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Existing LangGraph Agents with NeMo Agent Toolkit\n",
    "\n",
    "In this notebook, you'll learn how to integrate any existing LangGraph agent with NeMo Agent Toolkit using the `langgraph_wrapper` workflow type.\n",
    "\n",
    "We'll use LangGraph's Deep Research agent as a comprehensive example to demonstrate how you can wrap existing LangGraph agents so they work seamlessly with NeMo Agent Toolkit features like configurable LLMs, telemetry and observability with Phoenix, and comprehensive evaluation frameworks-all without refactoring the original agent code.\n",
    "\n",
    "The techniques shown here apply to any LangGraph agent, making it easy to add powerful capabilities provided by NeMo Agent Toolkit to your existing LangGraph applications.\n",
    "\n",
    "**Note:** The Deep Research agent is a complex multi-agent system that performs extensive web searches, planning, and synthesis. As a result, workflow execution may take several minutes per query. This is expected behavior due to the agent's thorough research methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "**Note:** This notebook runs from the NeMo Agent Toolkit repository root directory. All file paths are relative to the repo root.\n",
    "\n",
    "- [0.0) Setup](#setup)\n",
    "  - [0.1) Prerequisites](#prereqs)\n",
    "  - [0.2) API Keys](#api-keys)\n",
    "  - [0.3) Installing Dependencies](#installing-deps)\n",
    "- [1.0) About Our Example: The Deep Research Agent](#understanding-agent)\n",
    "- [2.0) Running the Agent with NeMo Agent Toolkit](#running-basic)\n",
    "  - [2.1) The Configuration File](#config-file)\n",
    "  - [2.2) Running Your First Query](#first-query)\n",
    "- [3.0) Making the Agent Configurable](#configurable-llms)\n",
    "  - [3.1) Understanding the Configurable Agent](#understanding-config)\n",
    "  - [3.2) Running with Different LLMs](#running-different-llms)\n",
    "- [4.0) Adding Telemetry with Phoenix](#telemetry)\n",
    "  - [4.1) Starting Phoenix](#starting-phoenix)\n",
    "  - [4.2) Running with Telemetry](#running-telemetry)\n",
    "  - [4.3) Viewing Traces in Phoenix](#viewing-traces)\n",
    "- [5.0) Evaluating Agent Performance](#evaluation)\n",
    "  - [5.1) Setting Up Evaluation](#setup-eval)\n",
    "  - [5.2) Running Evaluation](#running-eval)\n",
    "  - [5.3) Analyzing Results](#analyzing-results)\n",
    "- [6.0) Next Steps](#next-steps)\n",
    "\n",
    "<span style=\"color:rgb(0, 31, 153); font-style: italic;\">Note: In Google Colab use the Table of Contents tab to navigate.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Working Directory\n",
    "\n",
    "**This notebook is designed to run from the NeMo Agent Toolkit repository root directory.**\n",
    "\n",
    "All paths in this notebook are relative to the repository root. If you're running this notebook from a different location, the setup cells will automatically change to the repository root directory for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# 0.0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 0.1) Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Platform:** Linux, macOS, or Windows\n",
    "- **Python:** version 3.11, 3.12, or 3.13\n",
    "- **Python Packages:** `uv` (for package management)\n",
    "- **Docker:** (optional, for running Phoenix locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"api-keys\"></a>\n",
    "## 0.2) API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will need the following API keys:\n",
    "\n",
    "- **NVIDIA Build:** Obtain an NVIDIA Build API Key by creating an [NVIDIA Build](https://build.nvidia.com) account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "- **Tavily:** Obtain a Tavily API Key by creating a [Tavily](https://www.tavily.com/) account and generating a key at https://app.tavily.com/home (generous free tier available)\n",
    "- **Anthropic API Key** (optional): Required only for Section 2.0, which runs the original Deep Research agent with its default Claude model. You can skip Section 2.0 and start directly from Section 3.0 if you don't have an Anthropic API key.\n",
    "\n",
    "Then run the cell below to set your API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    tavily_api_key = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"installing-deps\"></a>\n",
    "## 0.3) Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install `uv`, which offers parallel downloads and faster dependency resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip_e2e_test"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now install NeMo Agent Toolkit with the LangChain subpackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip_e2e_test"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "uv pip show -q \"nvidia-nat-langchain\"\n",
    "if [ $? -ne 0 ]; then\n",
    "    uv pip install \"nvidia-nat[langchain]\"\n",
    "else\n",
    "    echo \"nvidia-nat[langchain] is already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to install the Deep Research agent dependencies. The Deep Research agent comes from LangChain's <!-- vale off -->[`Deepagent Quickstarts`](https://github.com/langchain-ai/deepagents-quickstarts) repository.<!-- vale on -->\n",
    "\n",
    "**Note:** This notebook is designed to run from the NeMo Agent Toolkit repository root. The cell below will ensure we're in the correct directory and install dependencies with paths relative to the repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Get the repository root directory\n",
    "repo_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode('utf-8').strip()\n",
    "\n",
    "# Change to the repository root\n",
    "os.chdir(repo_root)\n",
    "\n",
    "print(f\"Working directory set to: {os.getcwd()}\")\n",
    "print(f\"Verifying path exists: {os.path.exists('external/lc-deepagents-quickstarts/deep_research')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip_e2e_test"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install the deep_research dependencies\n",
    "# All paths are relative to the repo root\n",
    "uv pip install -e external/lc-deepagents-quickstarts/deep_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"understanding-agent\"></a>\n",
    "# 1.0) About Our Example: The Deep Research Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Our Example: The Deep Research Agent\n",
    "\n",
    "For this tutorial, we'll use LangGraph's Deep Research agent as our example. It's a sophisticated multi-agent system that showcases many advanced LangGraph patterns, making it an excellent demonstration of how to integrate complex LangGraph applications with NeMo Agent Toolkit.\n",
    "\n",
    "**Why This Example?** The Deep Research agent is feature-rich and demonstrates:\n",
    "- Multi-step workflows with planning and execution\n",
    "- Sub-agent coordination and parallel processing\n",
    "- Custom tool integration (Tavily search, strategic thinking)\n",
    "- File system operations and context management\n",
    "- State management across multiple agents\n",
    "\n",
    "These patterns are common in many LangGraph applications, so the integration techniques you'll learn here are widely applicable.\n",
    "\n",
    "### Deep Research Agent Features\n",
    "\n",
    "**Multi-Step Research Workflow:**\n",
    "1. Saves the research request\n",
    "2. Creates a structured plan with `TODO` items\n",
    "3. Delegates subtasks to specialized research sub-agents\n",
    "4. Synthesizes findings across multiple sources\n",
    "5. Responds with comprehensive analysis\n",
    "\n",
    "**Built-in DeepAgent Tools:**\n",
    "- `write_todos` and `read_todos`: Task planning and progress tracking\n",
    "- `ls`, `read_file`, `write_file`, `edit_file`: File system operations\n",
    "- `glob` and `grep`: File search and pattern matching\n",
    "- `task`: Sub-agent delegation for isolated context windows\n",
    "\n",
    "**Custom Research Tools:**\n",
    "- `tavily_search`: Web search that fetches full webpage content\n",
    "- `think_tool`: Strategic reflection mechanism for planning next steps\n",
    "\n",
    "**Sub-Agent Architecture:**\n",
    "The agent can spin up parallel research sub-agents (up to three concurrent) to investigate different aspects of a query simultaneously, with each sub-agent having its own isolated context window.\n",
    "\n",
    "### Original LangGraph Implementation\n",
    "\n",
    "The original Deep Research agent is defined in `external/lc-deepagents-quickstarts/deep_research/agent.py` and can be run using LangGraph's CLI through the `langgraph.json` [configuration file](https://docs.langchain.com/langsmith/cli#configuration-file):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"research\": \"./agent.py:agent\"\n",
    "  },\n",
    "  \"env\": \".env\"\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration tells LangGraph's CLI:\n",
    "- Where to find dependencies (current directory)\n",
    "- Where to find the agent graph (`agent.py:agent`)\n",
    "- Where to load environment variables (`.env` file)\n",
    "\n",
    "**The key insight:** The `langgraph_wrapper` workflow type provided by NeMo Agent Toolkit mimics this configuration pattern, allowing you to run most LangGraph agents that work with LangGraph CLI through NeMo Agent Toolkitâ€”while adding powerful new capabilities like telemetry, evaluation, and configurable LLMs.\n",
    "\n",
    "In the next section, we'll see exactly how this integration works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running-basic\"></a>\n",
    "# 2.0) Running the Agent with NeMo Agent Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"config-file\"></a>\n",
    "## 2.1) The Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo Agent Toolkit provides a `langgraph_wrapper` workflow type that allows you to integrate any existing LangGraph agent without modifying its code. Let's examine the basic configuration file:\n",
    "\n",
    "```yaml\n",
    "workflow:\n",
    "  _type: langgraph_wrapper\n",
    "  dependencies:\n",
    "    - external/lc-deepagents-quickstarts/deep_research\n",
    "  graph: external/lc-deepagents-quickstarts/deep_research/agent.py:agent\n",
    "  env: .env\n",
    "```\n",
    "\n",
    "This configuration closely mirrors LangGraph's `langgraph.json` format:\n",
    "\n",
    "| **LangGraph CLI** | **NeMo Agent Toolkit** | **Purpose** |\n",
    "|---|---|---|\n",
    "| `dependencies: [\".\"]` | `dependencies: [\"external/lc-deepagents-quickstarts/deep_research\"]` | Specifies Python packages to install |\n",
    "| `graphs.research: \"./agent.py:agent\"` | `graph: \"external/lc-deepagents-quickstarts/deep_research/agent.py:agent\"` | Points to the agent graph object |\n",
    "| `env: \".env\"` | `env: \".env\"` | Environment variables file |\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**NeMo Agent Toolkit advantages:**\n",
    "- Single unified configuration for workflows, LLMs, tools, and telemetry\n",
    "- Built-in support for evaluation and profiling\n",
    "- Automatic telemetry integration\n",
    "- Configurable LLM backends without code changes\n",
    "- Works seamlessly with other NeMo Agent Toolkit features\n",
    "\n",
    "Let's view the actual configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load examples/frameworks/auto_wrapper/langchain_deep_research/configs/config.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"first-query\"></a>\n",
    "## 2.2) Running Your First Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the Deep Research agent using NeMo Agent Toolkit. We'll start with a simple question to verify everything works correctly, then try a more complex research query.\n",
    "\n",
    "**Note about the LLM:** The Deep Research agent uses Anthropic's Claude model by default (hardcoded in the original `agent.py`). If you don't have access to an Anthropic API key or prefer to use a different model (such as Gemini or GPT-4), you can skip ahead to [Section 3.0: Making the Agent Configurable](#configurable-llms) where we show how to configure any LLM without modifying the agent code.\n",
    "\n",
    "### Quick Verification Query\n",
    "\n",
    "First, let's test with a simple question that should return quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    anthropic_api_key = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Verification Query\n",
    "\n",
    "First, let's test with a simple question that should return quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file examples/frameworks/auto_wrapper/langchain_deep_research/configs/config.yml \\\n",
    "  --input \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Behind the scenes, NeMo Agent Toolkit:\n",
    "1. Loaded the LangGraph agent from the specified Python module\n",
    "2. Installed the required dependencies automatically\n",
    "3. Set up the environment variables from the `.env` file\n",
    "4. Wrapped the agent to work within the NeMo Agent Toolkit execution framework\n",
    "5. Executed the query and streamed results back\n",
    "\n",
    "All of this happened **without modifying a single line of the original LangGraph agent code**!\n",
    "\n",
    "### Complex Research Query\n",
    "\n",
    "Now that we've verified the setup works, let's try a more complex research question. This query will demonstrate the agent's full capabilities:\n",
    "\n",
    "**Note:** This query involves web searches and synthesis, so it may take several minutes to complete. The agent will:\n",
    "1. Create a research plan with `TODO` items\n",
    "2. Delegate subtasks to research sub-agents\n",
    "3. Perform multiple web searches using Tavily\n",
    "4. Synthesize findings into a comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file examples/frameworks/auto_wrapper/langchain_deep_research/configs/config.yml \\\n",
    "  --input \"What are the key differences between ReAct and ReWOO agent architectures?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configurable-llms\"></a>\n",
    "# 3.0) Making the Agent Configurable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Deep Research agent hardcodes its LLM choice in the Python code. NeMo Agent Toolkit allows us to make the LLM configurable without modifying the core agent logic. This enables easy experimentation with different models, and supports choosing the right model and settings with the hyper-parameter optimizer included in NeMo Agent Toolkit (see [optimizer documentation](./../../../../docs/source/improve-workflows/optimizer.md)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"understanding-config\"></a>\n",
    "## 3.1) Understanding the Configurable Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the modified agent file that uses the Builder provided by NeMo Agent Toolkit to retrieve a configurable LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load examples/frameworks/auto_wrapper/langchain_deep_research/src/configurable_agent.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Changes from Original:\n",
    "\n",
    "**Original hardcoded LLM:**\n",
    "```python\n",
    "model = init_chat_model(model=\"anthropic:claude-sonnet-4-5-20250929\", temperature=0.0)\n",
    "```\n",
    "\n",
    "**Configurable version:**\n",
    "```python\n",
    "from nat.builder.sync_builder import SyncBuilder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "\n",
    "model = SyncBuilder.current().get_llm(\"agent\", wrapper_type=LLMFrameworkEnum.LANGCHAIN)\n",
    "```\n",
    "\n",
    "The `SyncBuilder.current().get_llm()` method:\n",
    "- Accesses the current builder instance via `SyncBuilder.current()`\n",
    "- Retrieves the LLM configuration named \"agent\" from the config file\n",
    "- Returns a LangChain-compatible model instance\n",
    "- Allows switching models without code changes\n",
    "\n",
    "Now let's look at the configuration file with LLM definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_llms.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `llms` section:\n",
    "\n",
    "```yaml\n",
    "llms:\n",
    "  agent:\n",
    "    _type: nim\n",
    "    model: nvidia/nemotron-3-nano-30b-a3b\n",
    "    max_tokens: 16384\n",
    "    chat_template_kwargs:\n",
    "      reasoning_budget: 1024\n",
    "```\n",
    "\n",
    "And the workflow now points to the configurable agent:\n",
    "\n",
    "```yaml\n",
    "workflow:\n",
    "  _type: langgraph_wrapper\n",
    "  dependencies:\n",
    "    - external/lc-deepagents-quickstarts/deep_research\n",
    "  graph: examples/frameworks/auto_wrapper/langchain_deep_research/src/configurable_agent.py:agent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running-different-llms\"></a>\n",
    "## 3.2) Running with Different LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily experiment with different models by just changing the configuration. Let's try running with `nemotron-3-nano-30b-a3b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_llms.yml \\\n",
    "  --input \"What are the trade-offs between using embeddings versus keywords for document retrieval?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try a different model, you can easily modify the config file or create a new one. For example, to use the `llama-3.3-nemotron-super-49b-v1` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/config_llama.yml\n",
    "\n",
    "llms:\n",
    "  agent:\n",
    "    _type: nim\n",
    "    model: nvidia/llama-3.3-nemotron-super-49b-v1\n",
    "    max_tokens: 16384\n",
    "workflow:\n",
    "  _type: langgraph_wrapper\n",
    "  dependencies:\n",
    "    - external/lc-deepagents-quickstarts/deep_research\n",
    "  graph: examples/frameworks/auto_wrapper/langchain_deep_research/src/configurable_agent.py:agent\n",
    "  env: .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file /tmp/config_llama.yml \\\n",
    "  --input \"What are the trade-offs between using embeddings versus keywords for document retrieval?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Configurable LLMs:\n",
    "\n",
    "1. **Easy Experimentation:** Test different models without code changes\n",
    "2. **A/B Testing:** Compare model performance on the same queries\n",
    "3. **Cost Optimization:** Switch between models based on cost and performance needs\n",
    "4. **Environment-Specific Models:** Use different models for dev, staging, and production\n",
    "5. **Unified Configuration:** All infrastructure choices in one place\n",
    "\n",
    "In the next section, we'll add telemetry to the agent to see how it performs with different models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"telemetry\"></a>\n",
    "# 4.0) Adding Telemetry with Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key benefits of using NeMo Agent Toolkit is the ability to add comprehensive instrumentation to any agent with just configuration changes. Let's add telemetry using Arize Phoenix, an open-source observability platform for LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"starting-phoenix\"></a>\n",
    "## 4.1) Starting Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to start the Phoenix server. Phoenix provides a web UI for viewing traces, spans, and metrics from your agent executions.\n",
    "\n",
    "**Option 1: Using Docker (Recommended)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Start Phoenix in the background using Docker\n",
    "docker run -d \\\n",
    "  --name phoenix \\\n",
    "  -p 6006:6006 \\\n",
    "  arizephoenix/phoenix:latest\n",
    "\n",
    "echo \"Phoenix is starting... It will be available at http://localhost:6006\"\n",
    "echo \"Give it a few seconds to fully initialize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Using Phoenix CLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install Phoenix\n",
    "uv pip install arize-phoenix\n",
    "\n",
    "# Start Phoenix server in the background\n",
    "# This will start the server on http://localhost:6006\n",
    "nohup phoenix serve > /dev/null 2>&1 &\n",
    "\n",
    "echo \"Phoenix server is starting at http://localhost:6006\"\n",
    "echo \"Give it a few seconds to fully initialize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Phoenix UI\n",
    "\n",
    "Once Phoenix is running, open your browser and navigate to:\n",
    "- **URL:** http://localhost:6006\n",
    "\n",
    "You should see the Phoenix dashboard. Initially, it will be empty since we haven't sent any traces yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running-telemetry\"></a>\n",
    "## 4.2) Running with Telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the configuration file that adds Phoenix telemetry. The key addition is the `general.telemetry` section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_telemetry.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The telemetry configuration is straightforward:\n",
    "\n",
    "```yaml\n",
    "general:\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "        _type: phoenix\n",
    "        endpoint: http://localhost:6006/v1/traces\n",
    "        project: lc_deepagents\n",
    "```\n",
    "\n",
    "This configuration:\n",
    "- Enables Phoenix tracing\n",
    "- Points to the local Phoenix server\n",
    "- Creates a project named `lc_deepagents` to organize traces\n",
    "\n",
    "Now let's run the agent with telemetry enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_telemetry.yml \\\n",
    "  --input \"Compare the performance characteristics of RAG versus fine-tuning for domain adaptation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"viewing-traces\"></a>\n",
    "## 4.3) Viewing Traces in Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the query completes, switch to your Phoenix UI (http://localhost:6006) and explore the telemetry data:\n",
    "\n",
    "### What You'll See in Phoenix:\n",
    "\n",
    "**1. Traces View:**\n",
    "- Complete execution trace of your agent run\n",
    "- Hierarchical view of all function calls and LLM interactions\n",
    "- Timing information for each step\n",
    "\n",
    "**2. Spans:**\n",
    "- Individual operations (LLM calls, tool calls, sub-agent delegations)\n",
    "- Input and output data for each operation\n",
    "- Latency and token usage metrics\n",
    "\n",
    "**3. Projects:**\n",
    "- All traces organized under the `lc_deepagents` project\n",
    "- Easy filtering and comparison of different runs\n",
    "\n",
    "**4. LLM Metrics:**\n",
    "- Token usage (prompt and completion tokens)\n",
    "- Cost estimates\n",
    "- Model performance statistics\n",
    "\n",
    "### Key Benefits of Telemetry:\n",
    "\n",
    "- **Debugging:** Trace exactly what your agent did at each step\n",
    "- **Performance Optimization:** Identify slow operations and bottlenecks\n",
    "- **Cost Monitoring:** Track token usage and API costs\n",
    "- **Quality Assurance:** Review agent decisions and tool usage patterns\n",
    "\n",
    "**Important:** Observability can be added with **zero code changes** to the original LangGraph agent! (although we changed one line in the original code to make the LLM configurable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "# 5.0) Evaluating Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful features of NeMo Agent Toolkit is its built-in evaluation framework. Let's set up systematic evaluation of our Deep Research agent using a dataset and automated metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup-eval\"></a>\n",
    "## 5.1) Setting Up Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the evaluation configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_eval.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Evaluation Configuration:\n",
    "\n",
    "**1. LLM Definitions:**\n",
    "```yaml\n",
    "llms:\n",
    "  agent:  # The LLM used by the research agent\n",
    "    _type: nim\n",
    "    model: nvidia/nemotron-3-nano-30b-a3b\n",
    "    max_tokens: 16384\n",
    "    chat_template_kwargs:\n",
    "      reasoning_budget: 1024\n",
    "    \n",
    "  judge:  # A separate LLM used to evaluate outputs\n",
    "    _type: nim\n",
    "    model: nvidia/nvidia-nemotron-nano-9b-v2\n",
    "```\n",
    "\n",
    "**2. Evaluation Dataset:**\n",
    "```yaml\n",
    "eval:\n",
    "  general:\n",
    "    dataset:\n",
    "      _type: csv\n",
    "      file_path: examples/frameworks/auto_wrapper/langchain_deep_research/data/DeepConsult_top1.csv\n",
    "      structure:\n",
    "        answer_key: baseline_answer\n",
    "```\n",
    "\n",
    "The dataset contains:\n",
    "- `question`: Research questions to answer\n",
    "- `baseline_answer`: Reference answers for comparison\n",
    "- `candidate_answer`: (populated during eval) Agent's responses\n",
    "\n",
    "**3. Evaluator Configuration:**\n",
    "```yaml\n",
    "evaluators:\n",
    "  judge:\n",
    "    _type: ragas\n",
    "    metric: AnswerAccuracy\n",
    "    llm_name: judge\n",
    "    input_obj_field: ground_truth\n",
    "```\n",
    "\n",
    "This uses RAGAS (Retrieval Augmented Generation Assessment) to evaluate:\n",
    "- **AnswerAccuracy:** How well the agent's answer matches the ground truth\n",
    "- Uses the \"judge\" LLM to score answers\n",
    "- Compares against the `ground_truth` field from the dataset\n",
    "\n",
    "Let's peek at the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('examples/frameworks/auto_wrapper/langchain_deep_research/data/DeepConsult_top1.csv')\n",
    "n_questions = len(df)\n",
    "sample_q = df['question'].iloc[0][:300] + \"...\" if len(df['question'].iloc[0]) > 300 else df['question'].iloc[0]\n",
    "sample_a = df['baseline_answer'].iloc[0][:500] + \"...\" if len(\n",
    "    df['baseline_answer'].iloc[0]) > 500 else df['baseline_answer'].iloc[0]\n",
    "\n",
    "display(\n",
    "    Markdown(f\"\"\"\n",
    "**Dataset contains:** `{n_questions}` **questions**\n",
    "\n",
    "---\n",
    "\n",
    "**Sample question:**\n",
    "```\n",
    "{sample_q}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Ground truth answer:**\n",
    "{sample_a}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running-eval\"></a>\n",
    "## 5.2) Running Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation using the `nat eval` command. This will:\n",
    "1. Load all questions from the dataset\n",
    "2. Run the Deep Research agent on each question\n",
    "3. Collect the agent's responses\n",
    "4. Use the judge LLM to evaluate answer quality\n",
    "5. Generate a comprehensive evaluation report\n",
    "\n",
    "**Note:** This may take a considerable amount of time depending on the dataset size, as each question involves:\n",
    "- Research planning\n",
    "- Multiple web searches\n",
    "- Sub-agent coordination\n",
    "- Synthesis and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat eval --config_file examples/frameworks/auto_wrapper/langchain_deep_research/configs/config_with_eval.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analyzing-results\"></a>\n",
    "## 5.3) Analyzing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the evaluation completes, NeMo Agent Toolkit generates several outputs in the configured `output_dir` (`.tmp/deepagents_eval`):\n",
    "\n",
    "### Output Files:\n",
    "\n",
    "**1. Judge Output (`judge_output.json`):**\n",
    "- Average evaluation score across all questions\n",
    "- Per-question scores and detailed reasoning\n",
    "- User input, agent response, and reference answer for each question\n",
    "- Structure:\n",
    "  ```json\n",
    "  {\n",
    "    \"average_score\": 0.5,\n",
    "    \"eval_output_items\": [\n",
    "      {\n",
    "        \"id\": \"\",\n",
    "        \"score\": 0.5,\n",
    "        \"reasoning\": {\n",
    "          \"user_input\": \"...\",\n",
    "          \"response\": \"...\",\n",
    "          \"reference\": \"...\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "**2. Workflow Output (`workflow_output.json`):**\n",
    "- Full agent responses for each question\n",
    "- Complete execution details\n",
    "- Raw agent output before evaluation\n",
    "\n",
    "Let's load and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load evaluation results from judge_output.json\n",
    "results_path = '.tmp/deepagents_eval/judge_output.json'\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    print(\"Evaluation Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average Score: {results.get('average_score', 'N/A')}\")\n",
    "    print(f\"Total Questions Evaluated: {len(results.get('eval_output_items', []))}\")\n",
    "\n",
    "    # Show per-question results\n",
    "    print(\"\\nPer-Question Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, item in enumerate(results.get('eval_output_items', [])):\n",
    "        print(f\"\\nQuestion {i+1}:\")\n",
    "        print(f\"  Score: {item.get('score', 'N/A')}\")\n",
    "\n",
    "        # Show reasoning details\n",
    "        reasoning = item.get('reasoning', {})\n",
    "        if reasoning:\n",
    "            user_input = reasoning.get('user_input', 'N/A')\n",
    "            print(f\"  User Input: {user_input[:100]}...\" if len(user_input) > 100 else f\"  User Input: {user_input}\")\n",
    "\n",
    "            # Show a snippet of the response and reference if available\n",
    "            if 'response' in reasoning:\n",
    "                response = str(reasoning['response'])[:200]\n",
    "                print(f\"  Agent Response (snippet): {response}...\")\n",
    "\n",
    "            if 'reference' in reasoning:\n",
    "                reference = str(reasoning['reference'])[:200]\n",
    "                print(f\"  Reference Answer (snippet): {reference}...\")\n",
    "else:\n",
    "    print(f\"Results file not found at {results_path}\")\n",
    "    print(\"Please ensure the evaluation has completed successfully.\")\n",
    "    print(\"\\nNote: Evaluation output is saved to:\")\n",
    "    print(\"  - judge_output.json: Evaluation scores and reasoning\")\n",
    "    print(\"  - workflow_output.json: Full agent responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing Different Models\n",
    "\n",
    "You can easily compare how different LLMs perform on the same evaluation dataset. Simply modify the `agent` LLM in the config and run evaluation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/config_eval_nemotron.yml\n",
    "\n",
    "llms:\n",
    "  agent:\n",
    "    _type: nim\n",
    "    model: nvidia/llama-3.3-nemotron-super-49b-v1\n",
    "    max_tokens: 16384\n",
    "  judge:\n",
    "    _type: nim\n",
    "    model: nvidia/nvidia-nemotron-nano-9b-v2\n",
    "    max_tokens: 16384\n",
    "\n",
    "workflow:\n",
    "  _type: langgraph_wrapper\n",
    "  dependencies:\n",
    "    - external/lc-deepagents-quickstarts/deep_research\n",
    "  graph: examples/frameworks/auto_wrapper/langchain_deep_research/src/configurable_agent.py:agent\n",
    "  env: .env\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: .tmp/deepagents_eval_nemotron\n",
    "    workflow_alias: deepagents_eval_nemotron\n",
    "    dataset:\n",
    "      _type: csv\n",
    "      file_path: examples/frameworks/auto_wrapper/langchain_deep_research/data/DeepConsult_top1.csv\n",
    "      structure:\n",
    "        answer_key: baseline_answer\n",
    "    profiler:\n",
    "      base_metrics: true\n",
    "\n",
    "  evaluators:\n",
    "    judge:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: judge\n",
    "      input_obj_field: ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with Nemotron\n",
    "!nat eval --config_file /tmp/config_eval_nemotron.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can compare the results between different models:\n",
    "- Check the respective output directories\n",
    "- Analyze cost versus quality trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits of NeMo Agent Toolkit Evaluation:\n",
    "\n",
    "1. **Systematic Testing:** Evaluate on consistent datasets\n",
    "2. **Automated Metrics:** Use LLM judges for quality assessment\n",
    "3. **Performance Tracking:** Monitor latency, tokens, and costs\n",
    "4. **Model Comparison:** Easily A/B test different LLMs\n",
    "5. **Regression Detection:** Catch quality degradation over time\n",
    "\n",
    "All achieved with **zero modifications** to the original LangGraph agent code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "# 6.0) Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've learned how to integrate a LangGraph Deep Research agent with NeMo Agent Toolkit and unlock powerful capabilities:\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "1. âœ… Set up and ran a complex LangGraph agent using NeMo Agent Toolkit\n",
    "2. âœ… Added comprehensive telemetry with Phoenix\n",
    "3. âœ… Made the agent configurable for different LLMs\n",
    "4. âœ… Evaluated agent performance systematically\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "\n",
    "**1. Additional Telemetry Backends:**\n",
    "- Try OpenTelemetry, Weave, or LangSmith\n",
    "- Configure multiple telemetry backends simultaneously\n",
    "- Set up alerting and monitoring\n",
    "\n",
    "**2. Advanced Evaluation:**\n",
    "- Add custom metrics beyond AnswerAccuracy\n",
    "- Use multiple judge LLMs for consensus scoring\n",
    "- Implement human-in-the-loop evaluation\n",
    "- Create evaluation reports with visualization\n",
    "\n",
    "**3. Performance Optimization:**\n",
    "- Use profiling to identify bottlenecks\n",
    "- Experiment with different model sizes\n",
    "- Optimize sub-agent delegation strategies\n",
    "- Implement caching for common queries\n",
    "\n",
    "**4. Production Deployment:**\n",
    "- Deploy the agent as a REST API using `nat serve`\n",
    "- Set up continuous evaluation pipelines\n",
    "- Implement version control for configurations\n",
    "\n",
    "**5. Custom Agent Development:**\n",
    "- Build your own agents using NeMo Agent Toolkit primitives\n",
    "- Integrate custom tools and functions\n",
    "- Implement domain-specific agent workflows\n",
    "- Create reusable agent templates\n",
    "\n",
    "### Learn More:\n",
    "\n",
    "- **NeMo Agent Toolkit Documentation:** https://docs.nvidia.com/nemo-agent-toolkit\n",
    "- **LangGraph Documentation:** https://langchain-ai.github.io/langgraph/\n",
    "- **Phoenix Documentation:** https://docs.arize.com/phoenix\n",
    "\n",
    "Happy agent building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
